# ğŸš€ IoT Live Stream Data Pipeline with PySpark, BigQuery & Airflow

This project implements a **real-time data processing pipeline** for IoT sensor data using **PySpark** on **Google Cloud Platform (GCP)**. The pipeline ingests live data from simulated devices, detects anomalies and harmful gas emissions, and stores enriched output for further analysis.

---

## ğŸŒ Data Source

- Simulated IoT devices deployed across **Hyderabad city**
- Devices continuously send sensor readings in **JSON format** to **Google Cloud Pub/Sub**
- Monitored metrics:
  - `Temperature`
  - `Pressure`
  - `COâ‚‚ levels`
  - `Harmful Gases` 

**Objective**: Monitor environmental health and detect hazardous conditions affecting public safety.

---

## ğŸ“¦ Key Features

- ğŸ”„ Real-time ingestion via **Cloud Pub/Sub**
- ğŸ§  Anomaly detection:
  - Constant/stuck `temperature` or `pressure`
- â˜ ï¸ Detection of **harmful gas emissions**
- âš ï¸ Identification of system/device failure based on patterns
- ğŸ“¤ Output destinations:
  - **Cloud Storage (GCS)** â€“ backup of enriched data
  - **BigQuery** â€“ for analytical dashboards

---

## ğŸ› ï¸ Technologies Used

| Category            | Stack                               |
|---------------------|--------------------------------------|
| **Cloud Provider**  | Google Cloud Platform (GCP)         |
| **Streaming Engine**| Pub/Sub, PySpark Structured Streaming|
| **Compute**         | Dataproc Cluster                    |
| **Storage**         | Cloud Storage, BigQuery             |
| **Orchestration**   | Apache Airflow (Cloud Composer)     |
| **Language**        | Python                              |

---

## ğŸ§  Processing Logic (PySpark)

- Read from **Pub/Sub** using PySpark
- Perform transformations and apply logic:
  - Validate schema
  - Detect environmental anomalies
  - Classify system failures
- Write transformed stream to **GCS and BigQuery**

---

## ğŸ›ï¸ Airflow Automation â€“ Cluster Scheduling

To automate resource management and reduce costs, **Airflow DAGs** are used to start and stop the **Dataproc cluster** daily:

### âœ… `start_cluster_dag.py`
- â° **Runs daily at 5:00 AM**
- ğŸš€ Starts the Dataproc cluster: `iot-processing-cluster`
- ğŸ”§ Uses: `DataprocStartClusterOperator`

### ğŸ›‘ `stop_cluster_dag.py`
- â° **Runs daily at 2:00 AM**
- ğŸ“´ Stops the cluster to avoid idle cost
- ğŸ”§ Uses: `DataprocStopClusterOperator`

> Airflow ensures the cluster runs **only during the active streaming window**, making the pipeline cost-effective and production-ready.

---

## ğŸ“‚ Project Structure

