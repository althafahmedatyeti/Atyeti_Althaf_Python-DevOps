# 🚀 IoT Live Stream Data Pipeline with PySpark, BigQuery & Airflow

This project implements a real-time data processing pipeline for IoT sensor data using **PySpark** on **Google Cloud Platform (GCP)**. The pipeline detects anomalies and system failures and writes the enriched data to **Google BigQuery** for downstream analytics.

---

## 🌍 Data Source

- The incoming data simulates **IoT devices deployed across Hyderabad city**.
- Devices monitor and send sensor data including:
  - `Temperature`
  - `Pressure`
  - `CO₂ levels`
  - `Harmful Gases (NO₂, SO₂)` ✅ *(newly added)*
- Data is published in **JSON format** to **Google Cloud Pub/Sub** via a Python-based generator running 24/7.
- **Purpose**: Monitor environmental conditions and detect harmful gas emissions for public health and safety.

---

## 📦 Features

- 🔄 Real-time ingestion from **Pub/Sub**
- 🧠 Anomaly detection:
  - Constant temperature (temp stuck)
  - Constant pressure (pressure stuck)
- ☠️ Detection of **harmful gas emissions**
- 🔧 System/device failure identification based on anomalies
- 📤 Writes enriched output to:
  - **Cloud Storage**
  - **BigQuery**


## 🛠️ Technologies Used

- **GCP Services**:
  - Cloud Pub/Sub
  - Cloud Storage (GCS)
  - Dataproc (PySpark Cluster)
  - BigQuery
- **PySpark Structured Streaming**
- **Airflow (Apache Airflow - Composer)**
- **Python**

---

## 🎛️ Airflow Automation for Cluster Scheduling

To optimize cost and resource management, **Airflow DAGs** have been added to automatically manage **Dataproc clusters**:

### ✅ `start_cluster_dag.py`
- 🔁 **Scheduled at 5:00 AM daily**
- 📌 Starts the `iot-processing-cluster` on Dataproc
- 🔧 Uses: `DataprocStartClusterOperator`

### 🛑 `stop_cluster_dag.py` *(Recommended)*for 
## 🛠️ Technologies Used

- **GCP Services**:
  - Cloud Pub/Sub
  - Cloud Storage (GCS)
  - Dataproc (PySpark Cluster)
  - BigQuery
- **PySpark Structured Streaming**
- **Airflow (Apache Airflow - Composer)**
- **Python**

---

## 🎛️ Airflow Automation for Cluster Scheduling

To optimize cost and resource management, **Airflow DAGs** have been added to automatically manage **Dataproc clusters**:

### ✅ `start_cluster_dag.py`
- 🔁 **Scheduled at 5:00 AM daily**
- 📌 Starts the `iot-processing-cluster` on Dataproc
- 🔧 Uses: `DataprocStartClusterOperator`

### 🛑 `stop_cluster_dag.py` *(Optional/Recommended)*
## 🛠️ Technologies Used

- **GCP Services**:
  - Cloud Pub/Sub
  - Cloud Storage (GCS)
  - Dataproc (PySpark Cluster)
  - BigQuery
- **PySpark Structured Streaming**
- **Airflow (Apache Airflow - Composer)**
- **Python**

---

## 🎛️ Airflow Automation for Cluster Scheduling

To optimize cost and resource management, **Airflow DAGs** have been added to automatically manage **Dataproc clusters**:

### ✅ `start_cluster_dag.py`
- 🔁 **Scheduled at 5:00 AM daily**
- 📌 Starts the `iot-processing-cluster` on Dataproc
- 🔧 Uses: `DataprocStartClusterOperator`

### 🛑 `stop_cluster_dag.py` *(Recommended)* for cost optimization
- 🔁 **Scheduled at 2:00 AM daily**
- 📌 Stops the cluster to reduce costs
- 🔧 Uses: `DataprocStopClusterOperator`

> Airflow enables automated orchestration, ensuring your cluster runs only during processing windows.
- 🔁 **Scheduled at 2:00 AM daily**
- 📌 Stops the cluster to reduce costs
- 🔧 Uses: `DataprocStopClusterOperator`

> Airflow enables automated orchestration, ensuring your cluster runs only during processing windows.
- 🔁 **Scheduled at 2:00 AM daily**
- 📌 Stops the cluster to reduce costs
- 🔧 Uses: `DataprocStopClusterOperator`

> Airflow enables automated orchestration, ensuring your cluster runs only during processing windows.
