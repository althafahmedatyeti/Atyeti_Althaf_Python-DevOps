# ðŸš€ IoT Live Stream Data Pipeline with PySpark, BigQuery & Airflow

This project implements a real-time data processing pipeline for IoT sensor data using **PySpark** on **Google Cloud Platform (GCP)**. The pipeline detects anomalies and system failures and writes the enriched data to **Google BigQuery** for downstream analytics.

---

## ðŸŒ Data Source

- The incoming data simulates **IoT devices deployed across Hyderabad city**.
- Devices monitor and send sensor data including:
  - `Temperature`
  - `Pressure`
  - `COâ‚‚ levels`
  - `Harmful Gases (NOâ‚‚, SOâ‚‚)` âœ… *(newly added)*
- Data is published in **JSON format** to **Google Cloud Pub/Sub** via a Python-based generator running 24/7.
- **Purpose**: Monitor environmental conditions and detect harmful gas emissions for public health and safety.

---

## ðŸ“¦ Features

- ðŸ”„ Real-time ingestion from **Pub/Sub**
- ðŸ§  Anomaly detection:
  - Constant temperature (temp stuck)
  - Constant pressure (pressure stuck)
- â˜ ï¸ Detection of **harmful gas emissions**
- ðŸ”§ System/device failure identification based on anomalies
- ðŸ“¤ Writes enriched output to:
  - **Cloud Storage**
  - **BigQuery**


## ðŸ› ï¸ Technologies Used

- **GCP Services**:
  - Cloud Pub/Sub
  - Cloud Storage (GCS)
  - Dataproc (PySpark Cluster)
  - BigQuery
- **PySpark Structured Streaming**
- **Airflow (Apache Airflow - Composer)**
- **Python**

---

## ðŸŽ›ï¸ Airflow Automation for Cluster Scheduling

To optimize cost and resource management, **Airflow DAGs** have been added to automatically manage **Dataproc clusters**:

### âœ… `start_cluster_dag.py`
- ðŸ” **Scheduled at 5:00 AM daily**
- ðŸ“Œ Starts the `iot-processing-cluster` on Dataproc
- ðŸ”§ Uses: `DataprocStartClusterOperator`

### ðŸ›‘ `stop_cluster_dag.py` *(Recommended)*for 
## ðŸ› ï¸ Technologies Used

- **GCP Services**:
  - Cloud Pub/Sub
  - Cloud Storage (GCS)
  - Dataproc (PySpark Cluster)
  - BigQuery
- **PySpark Structured Streaming**
- **Airflow (Apache Airflow - Composer)**
- **Python**

---

## ðŸŽ›ï¸ Airflow Automation for Cluster Scheduling

To optimize cost and resource management, **Airflow DAGs** have been added to automatically manage **Dataproc clusters**:

### âœ… `start_cluster_dag.py`
- ðŸ” **Scheduled at 5:00 AM daily**
- ðŸ“Œ Starts the `iot-processing-cluster` on Dataproc
- ðŸ”§ Uses: `DataprocStartClusterOperator`

### ðŸ›‘ `stop_cluster_dag.py` *(Optional/Recommended)*
## ðŸ› ï¸ Technologies Used

- **GCP Services**:
  - Cloud Pub/Sub
  - Cloud Storage (GCS)
  - Dataproc (PySpark Cluster)
  - BigQuery
- **PySpark Structured Streaming**
- **Airflow (Apache Airflow - Composer)**
- **Python**

---

## ðŸŽ›ï¸ Airflow Automation for Cluster Scheduling

To optimize cost and resource management, **Airflow DAGs** have been added to automatically manage **Dataproc clusters**:

### âœ… `start_cluster_dag.py`
- ðŸ” **Scheduled at 5:00 AM daily**
- ðŸ“Œ Starts the `iot-processing-cluster` on Dataproc
- ðŸ”§ Uses: `DataprocStartClusterOperator`

### ðŸ›‘ `stop_cluster_dag.py` *(Recommended)* for cost optimization
- ðŸ” **Scheduled at 2:00 AM daily**
- ðŸ“Œ Stops the cluster to reduce costs
- ðŸ”§ Uses: `DataprocStopClusterOperator`

> Airflow enables automated orchestration, ensuring your cluster runs only during processing windows.
- ðŸ” **Scheduled at 2:00 AM daily**
- ðŸ“Œ Stops the cluster to reduce costs
- ðŸ”§ Uses: `DataprocStopClusterOperator`

> Airflow enables automated orchestration, ensuring your cluster runs only during processing windows.
- ðŸ” **Scheduled at 2:00 AM daily**
- ðŸ“Œ Stops the cluster to reduce costs
- ðŸ”§ Uses: `DataprocStopClusterOperator`

> Airflow enables automated orchestration, ensuring your cluster runs only during processing windows.
